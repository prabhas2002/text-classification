{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preparing dataset and cleaing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df[['Summary','Score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary  Score\n",
       "0  Good Quality Dog Food      5\n",
       "1      Not as Advertised      1\n",
       "2  \"Delight\" says it all      4\n",
       "3         Cough Medicine      2\n",
       "4            Great taffy      5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Score'].replace({1: 'negative', 2: 'negative', 3: 'neutral', 4: 'positive', 5: 'positive'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    443777\n",
       "negative     82037\n",
       "neutral      42640\n",
       "Name: Score, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new['Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Summary    27\n",
       "Score       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.dropna(subset=['Summary'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Summary    0\n",
       "Score      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new['Summary'] = df_new['Summary'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting data into train,validation,test datasets\n",
    "X_tr=df_new.loc[:0.8*(len(df_new)),'Summary']\n",
    "Y_tr=df_new.loc[:0.8*(len(df_new)),'Score']\n",
    "X_dev=df_new.loc[0.8*(len(df_new)):0.9*(len(df_new)),'Summary']\n",
    "Y_dev=df_new.loc[0.8*(len(df_new)):0.9*(len(df_new)),'Score']\n",
    "X_te=df_new.loc[0.9*(len(df_new)):,'Summary']\n",
    "Y_te=df_new.loc[0.9*(len(df_new)):,'Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((454719,), (454719,), (56840,), (56840,), (56868,), (56868,))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr.shape,Y_tr.shape,  X_dev.shape,Y_dev.shape,   X_te.shape,Y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rule based classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specifying rules\n",
    "positive_words = ['large','delicious', 'amazing', 'excellent', 'fantastic', 'wonderful', 'tasty', 'awesome', 'great', 'love', 'perfect',\n",
    "                  'enjoyable', 'yum', 'outstanding', 'superb', 'best', 'heavenly', 'satisfying', 'mouthwatering', 'terrific', 'top-notch','yummy','good','nice']\n",
    "negative_words = ['small','not','disappointing', 'bad', 'poor', 'awful', 'horrible', 'mediocre', 'disgusting', 'terrible', 'unpleasant', 'bland',\n",
    "                  'overrated', 'subpar', 'unappetizing', 'disliked', 'unimpressive', 'displeasing', 'underwhelming', 'rancid', 'worse', 'regrettable']\n",
    "neutral_words = ['average', 'typical', 'ordinary', 'standard', 'common', 'usual', 'okay', 'acceptable', 'adequate', 'moderate',\n",
    "                 'indifferent', 'so-so', 'plain', 'routine', 'inoffensive', 'fair', 'balanced', 'unspectacular', 'neutral', 'decent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rule_model(X,Y):\n",
    "    #assigning scores\n",
    "    k=0\n",
    "    count=[0] * len(X)\n",
    "    for i in X:\n",
    "        words=i.split()\n",
    "        for word in words:\n",
    "            if word in positive_words:\n",
    "                count[k]+= 10\n",
    "            if word in negative_words:\n",
    "                count[k]-=10\n",
    "            if word in neutral_words:\n",
    "                count[k]+=5\n",
    "        k+=1\n",
    "        \n",
    "    #decision making    \n",
    "    for i in range(len(count)):\n",
    "        if count[i]>=10:\n",
    "            count[i]='positive'\n",
    "        elif count[i] >=0 and count[i] <=10:\n",
    "            count[i]='neutral'\n",
    "        else:\n",
    "            count[i]='negative'\n",
    "    \n",
    "    #convert data frame to list\n",
    "    ground_truth = Y.tolist()\n",
    "    \n",
    "    #accuracy for data\n",
    "    pred=0\n",
    "    for i in range(len(ground_truth)):\n",
    "        if ground_truth[i]==count[i]:\n",
    "            pred+=1\n",
    "    accuracy=pred/len(X)\n",
    "    \n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for train data is : 0.44286251509173796 \n",
      "accuracy for validation data is : 0.44799437016185784 \n",
      "accuracy for test data is : 0.44445030597172397 \n"
     ]
    }
   ],
   "source": [
    "print('accuracy for train data is : {0} ' .format(rule_model(X_tr,Y_tr)))\n",
    "print('accuracy for validation data is : {0} ' .format(rule_model(X_dev,Y_dev)))\n",
    "print('accuracy for test data is : {0} ' .format(rule_model(X_te,Y_te)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#even if we randomly assign each review to positive,neural,negative we can get 33% accuracy ,here accuracy is around 44% \n",
    "# HOW TO INCREASE ACCURACY?\n",
    "#make positive,negatuve,neutral lists effective and one can play with scores assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
